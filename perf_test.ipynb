{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for perf-testing PyTorch vs TensorFlow with and without GPU on a simple training set so I can figure out the best environment for training models. Here's the setup I used\n",
    "* Windows 11, i7-10 16GB RAM, RTX 2060 GPU w 6 GB RAM, VS Code\n",
    "* Fashion mnista data set to train a 9-layer CNN\n",
    "* Tensorflow w CUDA via WSL set up per https://www.tensorflow.org/install/pip\n",
    "* Tensorflow w CUDA via direct ML set up per https://learn.microsoft.com/en-us/windows/ai/directml/gpu-tensorflow-plugin\n",
    "\n",
    "Overall conclusion: Tensorflow w CUDA via direct ML was the winner by a wide margin\n",
    "\n",
    "Specific results when running the code below on various configurations above:\n",
    "1) CUDA was substantially faster than CPU on the 9 layer network. Experimentation not in the notebook shows the speedup depends on the network size; cpu was faster than cuda in a small 3-layer network, about 4x benefit on this 7 layer network, and 9x faster on a larger u-net. \n",
    "2) Tensorflow w CUDA via WSL (the recommended configuration) had all sorts of problems (see below), so I won't use this again until the tech matures.\n",
    "3) Tensorflow w CUDA via direct ML is the clear winner; good speed up, easy to code, just worked well\n",
    "4) PyTorch had similar training performance, but required 3x more code and had no clear upside, so this will not be my environment of choice.\n",
    "\n",
    "My experience on tensorflow CUDA via WSL (tensorflow.org's recommended configuration for Windows) was not good:\n",
    "* Setting it up was a PITA\n",
    "* After you are done, there are spurrious warnings about tensorRT and NUMA\n",
    "* WSL eats up a ton of disk space, and worse it eats up a ton or RAM when running, and worst of all it eats 1GB of RAM even when it is not running(!) due to virualization of the operation systems\n",
    "* It ran slower than CPU on my small models and crashed on larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Training on cpu. Number of model parameters: 620,682\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 17s 17ms/step - loss: 0.3684 - accuracy: 0.8662\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.2519 - accuracy: 0.9073\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.2162 - accuracy: 0.9205\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.1902 - accuracy: 0.9286\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.1708 - accuracy: 0.9358\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.1511 - accuracy: 0.9435\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.1381 - accuracy: 0.9481\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.1268 - accuracy: 0.9526\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.1112 - accuracy: 0.9582\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.0997 - accuracy: 0.9626\n",
      "CPU Training time: 165.61 seconds\n",
      "Training on GPU. Number of model parameters: 620,682\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 7s 6ms/step - loss: 0.3637 - accuracy: 0.8688\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2484 - accuracy: 0.9095\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2129 - accuracy: 0.9214\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1864 - accuracy: 0.9299\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1662 - accuracy: 0.9374\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1470 - accuracy: 0.9446\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1325 - accuracy: 0.9501\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1204 - accuracy: 0.9537\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1073 - accuracy: 0.9593\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1016 - accuracy: 0.9618\n",
      "GPU Training time: 54.91 seconds\n",
      "GPU:0 physical memory: {'current': 653702400, 'peak': 676082176}\n",
      "Test accuracy: 0.9152, loss: 0.3024\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "HAS_GPU = len (tf.config.list_physical_devices(\"GPU\")) > 0\n",
    "if HAS_GPU:\n",
    "    print(\"Available GPU devices:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test  = x_test/255.0\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    model = get_model()\n",
    "    print (f\"Training on cpu. Number of model parameters: {model.count_params():,d}\")\n",
    "    current_time = tf.timestamp()\n",
    "    model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    elapsed_time = tf.timestamp() - current_time\n",
    "    print (f\"CPU Training time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "if HAS_GPU:\n",
    "    model = get_model()\n",
    "    print (f\"Training on GPU. Number of model parameters: {model.count_params():,d}\")\n",
    "    current_time = tf.timestamp()\n",
    "    model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    elapsed_time = tf.timestamp() - current_time\n",
    "    print (f\"GPU Training time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"GPU:0 physical memory: {tf.config.experimental.get_memory_info('GPU:0')}\")\n",
    "\n",
    "# Print accuracy and loss on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}, loss: {test_loss:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it with PyTorch. Note that the code is *much* more complex as it requires:\n",
    "1) Manually computing the number of input parameters at each layer (since no built-in model.compile)\n",
    "2) Creating a training loop (since no built-in model.fit)\n",
    "3) Creating a custom dataset class, since the nn.mnist dataset loads images from disk each epoch, creating a significant performance bottleneck\n",
    "\n",
    "Given that PyTorch required about 3x as much coding to do the same thing, and has no clear benefits, it will not be my platform of choice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda GPU is available\n",
      "Training on cpu\n",
      "Number of parameters: 616074\n",
      "Epoch 1...Avg loss: 0.0149\n",
      "Epoch 2...Avg loss: 0.0137\n",
      "Epoch 3...Avg loss: 0.0133\n",
      "Epoch 4...Avg loss: 0.0131\n",
      "Epoch 5...Avg loss: 0.0129\n",
      "Epoch 6...Avg loss: 0.0128\n",
      "Epoch 7...Avg loss: 0.0125\n",
      "Epoch 8...Avg loss: 0.0123\n",
      "Epoch 9...Avg loss: 0.0122\n",
      "Epoch 10...Avg loss: 0.0120\n",
      "cpu Training time: 232.72 seconds\n",
      "Accuracy: 70.6%, Avg loss: 0.011809 \n",
      "\n",
      "Training on cuda\n",
      "Number of parameters: 616074\n",
      "Epoch 1...Avg loss: 0.0151\n",
      "Epoch 2...Avg loss: 0.0138\n",
      "Epoch 3...Avg loss: 0.0134\n",
      "Epoch 4...Avg loss: 0.0130\n",
      "Epoch 5...Avg loss: 0.0129\n",
      "Epoch 6...Avg loss: 0.0126\n",
      "Epoch 7...Avg loss: 0.0125\n",
      "Epoch 8...Avg loss: 0.0123\n",
      "Epoch 9...Avg loss: 0.0121\n",
      "Epoch 10...Avg loss: 0.0120\n",
      "cuda Training time: 29.95 seconds\n",
      "Accuracy: 70.2%, Avg loss: 0.011927 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from pandas import read_csv\n",
    "\n",
    "DIR = \"data/fashionmnist\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = read_csv(DIR + \"/fashion-mnist_train.csv\")\n",
    "test_data = read_csv(DIR + \"/fashion-mnist_train.csv\")\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    avg_loss = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        #print (X.shape)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        avg_loss += loss.item()\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return avg_loss / size\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epochs):\n",
    "    current_time = time.time()\n",
    "    for t in range(EPOCHS):\n",
    "        print(f\"Epoch {t+1}...\", end=\"\")\n",
    "        avg_loss = train_loop(dataloader, model, loss_fn, optimizer)\n",
    "        print(f\"Avg loss: {avg_loss:.4f}\")\n",
    "    elapsed_time = time.time() - current_time\n",
    "    print (f\"{device} Training time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Custom dataset that holds things in memory rather than loading from disc each time\n",
    "class FashionDataset(Dataset):\n",
    "    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"Method to initilaize variables.\"\"\" \n",
    "        self.fashion_MNIST = list(data.values)\n",
    "        \n",
    "        labels = []\n",
    "        images = []\n",
    "        \n",
    "        for i in self.fashion_MNIST:\n",
    "            label = i[0]\n",
    "            labels.append(label)\n",
    "\n",
    "            image = i[1:]\n",
    "            image = image/255.0\n",
    "            image = torch.FloatTensor(image).view(1, 28, 28)\n",
    "            images.append(image)\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = self.images[index]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "def evaluate():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=\"same\"),    # Output: 32x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),        # Output: 32x14x14; 14=28/2\n",
    "        nn.Conv2d(32, 64, kernel_size=3),   # Output: 64x12x12; 12=14-3+1\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),        # Output: 64x6x6; 6=12/2\n",
    "        nn.Flatten(),\n",
    "        nn.BatchNorm1d(64*6*6),\n",
    "        nn.Linear(64*6*6, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout1d(0.3),\n",
    "        nn.Linear(256, 10),\n",
    "    ).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    train_set = FashionDataset(train_data)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    model = model.to(device)\n",
    "    train(train_loader, model, loss_fn, optimizer, EPOCHS)\n",
    "\n",
    "    test_set = FashionDataset(test_data)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "devices = [\"cpu\"]\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda GPU is available\")\n",
    "    devices.append(\"cuda\")\n",
    "\n",
    "for device in devices:\n",
    "    print(f\"Training on {device}\")\n",
    "    evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
