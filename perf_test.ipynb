{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for perf-testing PyTorch vs TensorFlow with and without GPU on a simple training set so I can figure out the best environment for training models. Here's the setup I used\n",
    "* Windows 11, i7-10 16GB RAM, RTX 2060 GPU w 6 GB RAM, VS Code\n",
    "* Fashion mnista data set to train a 9-layer CNN\n",
    "* Tensorflow w CUDA via WSL set up per https://www.tensorflow.org/install/pip\n",
    "* Tensorflow w CUDA via direct ML set up per https://learn.microsoft.com/en-us/windows/ai/directml/gpu-tensorflow-plugin\n",
    "\n",
    "Overall conclusion: Tensorflow w CUDA via direct ML was the winner by a wide margin\n",
    "\n",
    "Specific results when running the code below on various configurations above:\n",
    "1) CUDA was substantially faster than CPU on the 9 layer network. Experimentation not in the notebook shows the speedup depends on the network size; cpu was faster than cuda in a small 3-layer network, about 4x benefit on this 7 layer network, and 9x faster on a larger u-net. \n",
    "2) Tensorflow w CUDA via WSL (the recommended configuration) had all sorts of problems (see below), so I won't use this again until the tech matures.\n",
    "3) Tensorflow w CUDA via direct ML is the clear winner; good speed up, easy to code, just worked well\n",
    "4) PyTorch trained a bit faster, but with less accuracy. It required 3x more code for the mixed results. So this will not be my environment of choice.\n",
    "\n",
    "My experience on tensorflow CUDA via WSL (tensorflow.org's recommended configuration for Windows) was not good:\n",
    "* Setting it up was a PITA\n",
    "* After you are done, there are spurrious warnings about tensorRT and NUMA\n",
    "* WSL eats up a ton of disk space, and worse it eats up a ton or RAM when running, and worst of all it eats 1GB of RAM even when it is not running(!) due to virualization of the operation systems\n",
    "* It ran slower than CPU on my small models and crashed on larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on cuda. Number of model parameters: 620,682\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 7s 6ms/step - loss: 0.3723 - accuracy: 0.8647\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.2527 - accuracy: 0.9072\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.2149 - accuracy: 0.9204\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.1870 - accuracy: 0.9304\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.1701 - accuracy: 0.9374\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.1518 - accuracy: 0.9434\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.1391 - accuracy: 0.9486\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.1234 - accuracy: 0.9527\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.1111 - accuracy: 0.9581\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.1053 - accuracy: 0.9593\n",
      "cuda Training time: 58.49 seconds\n",
      "GPU physical memory: {'current': 653773056, 'peak': 676817920}\n",
      "\n",
      "Test accuracy: 0.9222, loss: 0.2786\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device = \"cpu\"\n",
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    device = \"cuda\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test  = x_test/255.0\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "print(f\"Training on {device}. Number of model parameters: {model.count_params():,d}\")\n",
    "current_time = tf.timestamp()\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "elapsed_time = tf.timestamp() - current_time\n",
    "print (f\"{device} Training time: {elapsed_time:.2f} seconds\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU physical memory: {tf.config.experimental.get_memory_info('GPU:0')}\")\n",
    "print()\n",
    "\n",
    "# Print accuracy and loss on the test set\n",
    "total_loss, test_acc = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}, loss: {total_loss:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it with PyTorch. Note that the code is *much* more complex as it requires:\n",
    "1) Manually computing the number of input parameters at each layer (since no built-in model.compile)\n",
    "2) Creating a training loop (since no built-in model.fit)\n",
    "3) Creating a custom dataset class, since the nn.mnist dataset loads images from disk each epoch, creating a significant performance bottleneck\n",
    "\n",
    "Given that PyTorch required about 3x as much coding to do the same thing, and has no clear benefits, it will not be my platform of choice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on cuda\n",
      "Number of parameters: 616074\n",
      "Epoch 1 - loss: 0.0122, accuracy: 0.7054\n",
      "Epoch 2 - loss: 0.0115, accuracy: 0.7196\n",
      "Epoch 3 - loss: 0.0112, accuracy: 0.7255\n",
      "Epoch 4 - loss: 0.0111, accuracy: 0.7278\n",
      "Epoch 5 - loss: 0.0111, accuracy: 0.7288\n",
      "Epoch 6 - loss: 0.0111, accuracy: 0.7268\n",
      "Epoch 7 - loss: 0.0110, accuracy: 0.7299\n",
      "Epoch 8 - loss: 0.0109, accuracy: 0.7312\n",
      "Epoch 9 - loss: 0.0110, accuracy: 0.7310\n",
      "Epoch 10 - loss: 0.0110, accuracy: 0.7304\n",
      "cuda Training time: 31.63 seconds\n",
      "Accuracy: 72.5%, Avg loss: 0.011253 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from pandas import read_csv\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "DIR = \"data/fashionmnist\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Predict and compute loss and accuracy\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / size, correct / size\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epochs):\n",
    "    current_time = time.time()\n",
    "    for t in range(EPOCHS):\n",
    "        print(f\"Epoch {t+1} - \", end=\"\")\n",
    "        loss, accuracy = train_loop(dataloader, model, loss_fn, optimizer)\n",
    "        print(f\"loss: {loss:.4f}, accuracy: {accuracy:.4f}\")\n",
    "    elapsed_time = time.time() - current_time\n",
    "    print (f\"{device} Training time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    total_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {total_loss:>8f} \\n\")\n",
    "\n",
    "# A datasets.MNIST wrapper that holds normalized images in memory rather than loading from disc each epoch\n",
    "# Annoying that Pytorch doesn't have this built in (like tensorflow does)\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "class FashionDataset(Dataset):\n",
    "    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, dir, train):\n",
    "        \"\"\"Method to initilaize variables.\"\"\" \n",
    "        dataset = datasets.MNIST(dir, download=True, train=train)\n",
    "\n",
    "        labels = []\n",
    "        images = []\n",
    "\n",
    "        # Iterate over each image and lable in the dataset\n",
    "        for item in list(dataset):\n",
    "            label = item[1]\n",
    "            labels.append(label)\n",
    "            image = np.asarray(item[0])\n",
    "            image = image/255.0\n",
    "            image = torch.FloatTensor(image).view(1, 28, 28)\n",
    "            images.append(image)\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = self.images[index]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def evaluate():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=\"same\"),    # Output: 32x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),        # Output: 32x14x14; 14=28/2\n",
    "        nn.Conv2d(32, 64, kernel_size=3),   # Output: 64x12x12; 12=14-3+1\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),        # Output: 64x6x6; 6=12/2\n",
    "        nn.Flatten(),\n",
    "        nn.BatchNorm1d(64*6*6),\n",
    "        nn.Linear(64*6*6, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout1d(0.3),\n",
    "        nn.Linear(256, 10),\n",
    "    ).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    train_set = FashionDataset(DIR, train=True)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    model = model.to(device)\n",
    "    train(train_loader, model, loss_fn, optimizer, EPOCHS)\n",
    "\n",
    "    test_set = FashionDataset(DIR, train=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "print(f\"Training on {device}\")\n",
    "evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
