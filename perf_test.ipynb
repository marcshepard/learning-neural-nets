{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for perf-testing PyTorch vs TensorFlow with and without GPU on a simple training set so I can figure out the best environment for training models. Here's the setup I used\n",
    "* Windows 11, i7-10 16GB RAM, RTX 2060 GPU w 6 GB RAM, VS Code\n",
    "* Fashion mnista data set to train a 9-layer CNN\n",
    "* Tensorflow w CUDA via WSL set up per https://www.tensorflow.org/install/pip\n",
    "* Tensorflow w CUDA via direct ML set up per https://learn.microsoft.com/en-us/windows/ai/directml/gpu-tensorflow-plugin\n",
    "* PyTorch w CUDA set up per https://pytorch.org/get-started/locally/. I had to use pip install (conda install had missing DLLs), and I had to install this before the tensorflow w CUDA via direct ML to get them to both work together.\n",
    "\n",
    "Overall conclusion: PyTorch w CUDA was the winner by a wide margin, although required a lot more code\n",
    "\n",
    "Specific results when running the code below on various configurations above:\n",
    "1) CUDA was substantially faster than CPU (~4x) on this 9 layer CNN. I saw 9x speed up on a larger unet.\n",
    "2) Tensorflow w CUDA via WSL (the recommended configuration) had problems (see below), so I won't use this again until the tech matures.\n",
    "3) Tensorflow w CUDA via direct ML was better than 1 and 2; good speed up, easy to code, just worked well\n",
    "4) PyTorch trained the fastest. It required 3x more code, but I think this is a good thing as it forces me to think.\n",
    "\n",
    "My experience on tensorflow CUDA via WSL (tensorflow.org's recommended configuration for Windows) was not good:\n",
    "* Setting it up was a PITA\n",
    "* After you are done, there are spurrious warnings about tensorRT and NUMA\n",
    "* WSL eats up a ton of disk space, and worse it eats up a ton or RAM when running, and worst of all it eats 1GB of RAM even when it is not running(!) due to virualization of the operation systems\n",
    "* It ran slower than CPU on my small models and crashed on larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on cuda. Number of model parameters: 620,682\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 7s 6ms/step - loss: 0.3606 - accuracy: 0.8706\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2447 - accuracy: 0.9097\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2065 - accuracy: 0.9229\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1802 - accuracy: 0.9336\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1566 - accuracy: 0.9414\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1382 - accuracy: 0.9481\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1217 - accuracy: 0.9553\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1115 - accuracy: 0.9591\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1046 - accuracy: 0.9612\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0907 - accuracy: 0.9658\n",
      "cuda Training time: 55.86 seconds\n",
      "GPU physical memory: {'current': 653294080, 'peak': 676936704}\n",
      "\n",
      "Test accuracy: 0.9142, loss: 0.3749\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device = \"cpu\"\n",
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    device = \"cuda\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test  = x_test/255.0\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "print(f\"Training on {device}. Number of model parameters: {model.count_params():,d}\")\n",
    "current_time = tf.timestamp()\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "elapsed_time = tf.timestamp() - current_time\n",
    "print (f\"{device} Training time: {elapsed_time:.2f} seconds\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU physical memory: {tf.config.experimental.get_memory_info('GPU:0')}\")\n",
    "print()\n",
    "\n",
    "# Print accuracy and loss on the test set\n",
    "total_loss, test_acc = model.evaluate(x_test,  y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}, loss: {total_loss:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it with PyTorch. Note that the code is *much* more complex as it requires:\n",
    "1) Manually computing the number of input parameters at each layer (since no built-in model.compile)\n",
    "2) Creating a training loop (since no built-in model.fit)\n",
    "3) Creating a custom dataset class, since the nn.mnist dataset loads images from disk each epoch, creating a significant performance bottleneck\n",
    "\n",
    "Given that PyTorch required about 3x as much coding to do the same thing, and has no clear benefits, it will not be my platform of choice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on cuda\n",
      "Number of parameters: 611914\n",
      "Epoch 1 - accuracy: 86.9%, loss: 0.3659\n",
      "Epoch 2 - accuracy: 90.3%, loss: 0.2611\n",
      "Epoch 3 - accuracy: 91.6%, loss: 0.2249\n",
      "Epoch 4 - accuracy: 92.6%, loss: 0.2023\n",
      "Epoch 5 - accuracy: 93.2%, loss: 0.1833\n",
      "Epoch 6 - accuracy: 93.8%, loss: 0.1665\n",
      "Epoch 7 - accuracy: 94.3%, loss: 0.1520\n",
      "Epoch 8 - accuracy: 94.8%, loss: 0.1387\n",
      "Epoch 9 - accuracy: 95.3%, loss: 0.1264\n",
      "Epoch 10 - accuracy: 95.6%, loss: 0.1183\n",
      "cuda Training time: 32.45 seconds\n",
      "Evaluating the model on the test dataset:\n",
      "Accuracy: 93.0%, Avg loss: 0.2090 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from pandas import read_csv\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "DIR = \"data/fashionmnist\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss, correct = 0, 0\n",
    "    batches = 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        batches += 1\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Predict and compute loss and accuracy\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = total_loss / batches\n",
    "    accuracy = correct / size\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epochs):\n",
    "    current_time = time.time()\n",
    "    for t in range(EPOCHS):\n",
    "        print(f\"Epoch {t+1} - \", end=\"\")\n",
    "        loss, accuracy = train_loop(dataloader, model, loss_fn, optimizer)\n",
    "        print(f\"accuracy: {accuracy * 100:>0.1f}%, loss: {loss:.4f}\")\n",
    "    elapsed_time = time.time() - current_time\n",
    "    print (f\"{device} Training time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    batches = 0\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            batches += 1\n",
    "    avg_loss = total_loss / batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_loss:.4f} \\n\")\n",
    "\n",
    "# A datasets.MNIST wrapper that holds normalized images in memory rather than loading from disc each epoch\n",
    "# Annoying that Pytorch doesn't have this built in (like tensorflow does)\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "class FashionDataset(Dataset):\n",
    "    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, dir, train):\n",
    "        \"\"\"Method to initilaize variables.\"\"\" \n",
    "        dataset = datasets.FashionMNIST(dir, download=True, train=train)\n",
    "\n",
    "        labels = []\n",
    "        images = []\n",
    "\n",
    "        # Iterate over each image and lable in the dataset\n",
    "        for item in list(dataset):\n",
    "            label = item[1]\n",
    "            labels.append(label)\n",
    "            image = np.asarray(item[0])\n",
    "            image = image/255.0\n",
    "            image = torch.FloatTensor(image).view(1, 28, 28)\n",
    "            images.append(image)\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = self.images[index]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def evaluate():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=\"same\"),    # Output: 32x28x28\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),        # Output: 32x14x14; 14=28/2\n",
    "        nn.Conv2d(32, 64, kernel_size=3),   # Output: 64x12x12; 12=14-3+1\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),        # Output: 64x6x6; 6=12/2\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(64*6*6, 256, bias=False),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 10),\n",
    "    ).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    train_set = FashionDataset(DIR, train=True)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    model = model.to(device)\n",
    "    train(train_loader, model, loss_fn, optimizer, EPOCHS)\n",
    "\n",
    "    print (\"Evaluating the model on the test dataset:\")\n",
    "    test_set = FashionDataset(DIR, train=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "    model.train(False)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "print(f\"Training on {device}\")\n",
    "evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
