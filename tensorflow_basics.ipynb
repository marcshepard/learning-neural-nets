{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This notebook is for exploring tensorflow basics; tensors, variables, autograd, graphs/functions, modules/layers/modules\n","\n","A tensor (tf.constant) is an immutable multi-dimensional array, optimized for GPU processing\n","\n","A variable (tf.variable) is a mutable tensor\n","\n","Autograd computes the gradients of variables automatically; you first record operations on a GradientTape. Autograd then allows you to find the gradient of any of the input tensors with respect to a computed value, after recording the operations on the GradientTape.\n","\n","A function (tf.function) is an efficient way to process tensorts. Tensorflow can execute a tf.Graph, which is an efficient DAG for performing tf.Operations (internal nodes) on tf.Tensors (inputs) to produce a single tf.Tensor (output). Graphs are created from Python code, but don't require the Python runtime and are much more efficient. The object model, per https://www.tensorflow.org/guide/function, is:\n","* tf.function wraps a Python function, returning a Function object\n","* A Function manages a cache of ConcreteFunctions and picks the most specific match based on the signature of inputs, or performs tracing to create a tfGraph and wrap it in a ConcreteFunction if there is no match\n","* A ConcreteFunction wraps a tf.Graph\n","* A tf.Graph is the raw, language-agnostic, portable representation of a TensorFlow computation\n","Tracing captures the tensorflow operations into a graph, converts some python code (e.g., numpy operations) into tensorflow first, and discards other operations like \"print\" statements. By default a Function will execute a graph if it can. A tf.function can also run \"eagerly\", where it executes the underlying python code rather than the graph.\n","\n","A Model is a series of Layers. Both Models and Layers are typically implemented as subclasses of tf.Module, which is a container of tf.Variables (trainable parameters) and other tf.Modules (layers).\n","\n","In practice, one normally uses Keras. The tf.keras.layers.Layer subclasses tf.Module and adds a lot of functionality. That is a topic for another notebook."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","import timeit\n","from datetime import datetime\n","from os import listdir, remove"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A tensor (tf.constant) is an immutable multi-dimensional array, optimized for GPU processing"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1. 2.]\n"," [3. 4.]] @ [[1.]\n"," [2.]] = [[ 5.]\n"," [11.]]\n","Dimensions of c: (2, 1)\n","Type of c: <dtype: 'float32'>\n"]}],"source":["# Tensor basics\n","\n","# A 2x2 matrix\n","a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n","\n","# A 2x1 variable\n","b = tf.constant([[1.0], [2.0]])\n","\n","c = a @ b\n","print (f\"{a} @ {b} = {c}\")\n","print (f\"Dimensions of c: {c.shape}\")\n","print (f\"Type of c: {c.dtype}\")\n","\n","c = tf.Variable(c)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A variable (tf.variable) is a mutable tensor"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["a = <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>\n","type of a: <dtype: 'float32'>\n","a = <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n","type of a: <dtype: 'float32'>\n"]}],"source":["# Variable basics - just like tensors, but can be modified\n","\n","a = tf.Variable([2.0, 3.0])\n","print (f\"a = {a}\")\n","print (f\"type of a: {a.dtype}\")\n","\n","# Reassigning values doesn't change the dtype\n","a.assign([1, 2])\n","print (f\"a = {a}\")\n","print (f\"type of a: {a.dtype}\")\n","\n","# Note: You can't assign a new shape - only modify values"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Autograd computes the gradients of variables automatically; you first record operations on a GradientTape. Autograd then allows you to find the gradient of any of the input tensors with respect to a computed value, after recording the operations on the GradientTape."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dl_dw = [[ 1.8597615 -0.3769089]\n"," [ 3.719523  -0.7538178]\n"," [ 5.5792847 -1.1307267]]\n","dl_db = [ 1.8597615 -0.3769089]\n","tf.Tensor(6.0, shape=(), dtype=float32)\n","None\n","None\n","None\n","x0 before applying gradient: <tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>\n","x0 after: <tf.Variable 'x0:0' shape=() dtype=float32, numpy=2.4>\n"]}],"source":["# Autograd allows you to compute gradients of input tensorts wrt output tensors automatically\n","\n","w = tf.Variable(tf.random.normal((3, 2)), name='w')\n","b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n","x = [[1., 2., 3.]]\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  y = x @ w + b\n","  loss = tf.reduce_mean(y**2)\n","\n","# Note, the 2nd parameter can be an iterable (list, dictionary, etc), and the gradients will be computed for each element\n","# and returned as the same iterable type\n","[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n","\n","print (f\"dl_dw = {dl_dw}\")\n","print (f\"dl_db = {dl_db}\")\n","\n","# Note: By default, only tf.Variable objects are recorded in the tape. To record tensors, you must use tape.watch()\n","# You can mark a tf.Variable as untrainable by setting trainable=False\n","x0 = tf.Variable(3.0, name='x0')                  # A trainable variable\n","x1 = tf.Variable(3.0, name='x1', trainable=False) # Not trainable\n","x2 = tf.Variable(2.0, name='x2') + 1.0            # Not a Variable: A variable + tensor returns a tensor.\n","x3 = tf.constant(3.0, name='x3')                  # Not a Variable\n","\n","with tf.GradientTape() as tape:\n","  y = (x0**2) + (x1**2) + (x2**2)\n","\n","grad = tape.gradient(y, [x0, x1, x2, x3])\n","\n","for g in grad:  # So only the first gradient is computed\n","  print(g)\n","\n","# Apply the gradient to x0 (the only trainable variable)\n","print (\"x0 before applying gradient:\", x0)\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n","optimizer.apply_gradients(zip(grad, [x0]))\n","print (\"x0 after:\", x0)\n","\n","# Note: In practice, you'll train Modules (more on those later), which specify what is trainable in Module.trainable_variables"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A function (tf.function) is an efficient way to process tensorts. Tensorflow can execute a tf.Graph, which is an efficient DAG for performing tf.Operations (internal nodes) on tf.Tensors (inputs) to produce a single tf.Tensor (output). Graphs are created from Python code, but don't require the Python runtime and are much more efficient. The object model, per https://www.tensorflow.org/guide/function, is:\n","* tf.function wraps a Python function, returning a Function object\n","* A Function manages a cache of ConcreteFunctions and picks the most specific match based on the signature of inputs, or performs tracing to create a tfGraph and wrap it in a ConcreteFunction if there is no match\n","* A ConcreteFunction wraps a tf.Graph\n","* A tf.Graph is the raw, language-agnostic, portable representation of a TensorFlow computation\n","Tracing captures the tensorflow operations into a graph, converts some python code (e.g., numpy operations) into tensorflow first, and discards other operations like \"print\" statements. By default a Function will execute a graph if it can. A tf.function can also run \"eagerly\", where it executes the underlying python code rather than the graph."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["This should print output, due to tracing\n","<tf.Variable 'Variable:0' shape=() dtype=int32> <tf.Variable 'Variable:0' shape=() dtype=int32>\n","No output, since the Function now has a cached graph\n","This should print output, since it's being called with a different signature\n","<tf.Variable 'Variable:0' shape=() dtype=float32> <tf.Variable 'Variable:0' shape=() dtype=float32>\n"]}],"source":["# One way to declare a Function is with a decorator\n","@tf.function\n","def add (x, y):\n","    print (x, y)\n","    return x+y\n","\n","# Let's see it execute eagerly and in graph model\n","print (\"This should print output, due to tracing\")\n","a = tf.Variable(1)\n","b = tf.Variable(2)\n","af = tf.Variable(1.)\n","bf = tf.Variable(2.)\n","assert (add (a, b) == 3)\n","\n","print (\"No output, since the Function now has a cached graph\")\n","assert (add (a, b) == 3)\n","\n","print (\"This should print output, since it's being called with a different signature\")\n","assert (add (af, bf) == 3.0)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Regular function execution: 4.442750700000033 seconds\n","tf.function execution: 0.5394228999998631 seconds\n","tf.function executing eagerly: 4.387751000000208 seconds\n"]}],"source":["# Graph execution is much faster if there are many small functions, like below\n","# Less so if there are few large functions\n","x = tf.random.uniform(shape=[10, 10], minval=-1, maxval=2, dtype=tf.dtypes.int32)\n","\n","def power(x, y):\n","  result = tf.eye(10, dtype=tf.dtypes.int32)\n","  for _ in range(y):\n","    result = tf.matmul(x, result)\n","  return result\n","\n","print(\"Regular function execution:\", timeit.timeit(lambda: power(x, 100), number=1000), \"seconds\")\n","\n","# You can force a Function to execute eagerly for test purposes\n","power_as_graph = tf.function(power)\n","print(\"tf.function execution:\", timeit.timeit(lambda: power_as_graph(x, 100), number=1000), \"seconds\")\n","\n","# You can force a Function to execute eagerly for test purposes\n","tf.config.run_functions_eagerly(True)\n","print(\"tf.function executing eagerly:\", timeit.timeit(lambda: power_as_graph(x, 100), number=1000), \"seconds\")\n","tf.config.run_functions_eagerly(False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","A Model is a series of Layers. Both Models and Layers are typically implemented as subclasses of tf.Module, which is a container of tf.Variables (trainable parameters) and other tf.Modules (layers).\n","Models have the following common properties:\n","* submodules - collection of submodules\n","* variables/trainable_variables/non_trainable_variables - uses reflection, so not very performant\n","Variables can be marked trainable or non-trainable when declared, as well as via the Model.trainable_variables property\n","\n","If the __call__ method is a tf.Function, then the model will generate tf.Graphs for faster execution and can be saved and run without Python."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n","array([[ 0.11862059, -0.6205737 ],\n","       [ 0.47175336, -0.7345508 ],\n","       [-0.24489878, -0.2893617 ]], dtype=float32)>)\n","(<tf.Variable 'u:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,)\n","Model results: tf.Tensor([[0.69631386 0.        ]], shape=(1, 2), dtype=float32)\n","Submodules: (<__main__.DenseLayer object at 0x000002253B661960>, <__main__.DenseLayer object at 0x000002253B663C10>)\n","<tf.Variable 'b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)> \n","\n","<tf.Variable 'u:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)> \n","\n","<tf.Variable 'w:0' shape=(3, 3) dtype=float32, numpy=\n","array([[ 1.8356816, -1.3086845, -0.7664374],\n","       [ 1.0469161,  0.8300276,  1.0368861],\n","       [-0.5955064,  3.1537387, -1.3027871]], dtype=float32)> \n","\n","<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> \n","\n","<tf.Variable 'u:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> \n","\n","<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n","array([[-0.17066126,  0.1288354 ],\n","       [ 0.27605692, -2.7399561 ],\n","       [ 0.96472883, -0.72754717]], dtype=float32)> \n","\n"]}],"source":["class DenseLayer(tf.Module):\n","  def __init__(self, input_dim, output_dim, name=None):\n","    super().__init__(name=name)\n","    self.w = tf.Variable(\n","      tf.random.normal([input_dim, output_dim]), name='w')\n","    self.b = tf.Variable(tf.zeros([output_dim]), name='b', dtype=tf.float32)\n","    self.u = tf.Variable(tf.zeros([output_dim]), name='u', trainable=False, dtype=tf.float32) # Just to show how to make a non-trainable variable\n","  def __call__(self, x):\n","    y = tf.matmul(x, self.w) + self.b + self.u\n","    return tf.nn.relu(y)\n","  \n","d = DenseLayer(input_dim=3, output_dim=2)\n","d(tf.ones([1, 3]))\n","print (d.trainable_variables)\n","print (d.non_trainable_variables)\n","\n","class MySequentialModule(tf.Module):\n","  def __init__(self, name=None):\n","    super().__init__(name=name)\n","\n","    self.dense_1 = DenseLayer(input_dim=3, output_dim=3)\n","    self.dense_2 = DenseLayer(input_dim=3, output_dim=2)\n","\n","  @ tf.function\n","  def __call__(self, x):\n","    x = self.dense_1(x)\n","    return self.dense_2(x)\n","\n","# You have made a model with two layers\n","my_model = MySequentialModule(name=\"the_model\")\n","\n","# Call it, with random results\n","print(\"Model results:\", my_model(tf.constant([[2.0, 2.0, 2.0]])))\n","\n","print(\"Submodules:\", my_model.submodules)\n","\n","for var in my_model.variables:\n","  print(var, \"\\n\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Modules can be checkpointed. Checkpointing allows you to save the weights so you can resume training later. Checkpointing consist of two kinds of files: the data itself and an index file for metadata. The index file keeps track of what is actually saved and the numbering of checkpoints, while the checkpoint data contains the variable values and their attribute lookup paths."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original model results: tf.Tensor([[2.3431063 0.       ]], shape=(1, 2), dtype=float32)\n","Restored model results: tf.Tensor([[2.3431063 0.       ]], shape=(1, 2), dtype=float32)\n"]}],"source":["print(\"Original model results:\", my_model(tf.constant([[1.0, 2.0, 3.0]])))\n","chkp_path = \"data/models/my_checkpoint\"\n","checkpoint = tf.train.Checkpoint(model=my_model)\n","checkpoint.write(chkp_path)\n","\n","# Inspect the checkpoint\n","tf.train.list_variables(chkp_path)\n","\n","# When you load the model back in, you overwrite the values in the existing model\n","new_model = MySequentialModule(name=\"restored_model\")\n","new_checkpoint = tf.train.Checkpoint(model=new_model)\n","new_checkpoint.restore(chkp_path)\n","\n","# Should be the same result as above\n","print (\"Restored model results:\", new_model(tf.constant([[1.0, 2.0, 3.0]])))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Models can be saved and loaded"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor([[0.      4.12147]], shape=(1, 2), dtype=float32)\n"]},{"data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 9580), started 0:31:11 ago. (Use '!kill 9580' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-733cb4b3a3552aa6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-733cb4b3a3552aa6\");\n","          const url = new URL(\"http://localhost\");\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Since __call__ has been defined as a tf.function, it will generate a graph we can visualize within a TensorBoard summary\n","\n","# First set up logging after clearing prior logs\n","logdir = \"logs/tensorboard_test\"\n","for file in listdir(logdir):\n","  remove(logdir + \"/\" + file)\n","writer = tf.summary.create_file_writer(logdir)\n","\n","# Create a new model to get a fresh trace, otherwise the summary will not see the graph.\n","new_model = MySequentialModule()\n","\n","# Bracket the function call with tf.summary.trace_on() and tf.summary.trace_export().\n","tf.summary.trace_on(graph=True)\n","z = print(new_model(tf.constant([[2.0, 2.0, 2.0]])))\n","with writer.as_default():\n","  tf.summary.trace_export(\n","      name=\"my_func_trace\",\n","      step=0,\n","      profiler_outdir=logdir)\n","\n","# View the output in TensorBoard\n","%load_ext tensorboard\n","%tensorboard --logdir logs/tensorboard_test"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A SavedModel is the recommended way of sharing trained models. A SavedModel contains both a collection of functions and a collection of weights"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: data/models/saved_model\\assets\n","Files in the saved_model folder: ['assets', 'fingerprint.pb', 'saved_model.pb', 'variables']\n","Original model results: tf.Tensor([[2.3431063 0.       ]], shape=(1, 2), dtype=float32)\n","Restored model results: tf.Tensor([[2.3431063 0.       ]], shape=(1, 2), dtype=float32)\n"]}],"source":["model_path = \"data/models/saved_model\"\n","tf.saved_model.save(my_model, model_path)\n","\n","# Enumerate all files in the saved_model directory\n","print (\"Files in the saved_model folder:\", listdir(model_path))\n","\n","# Load the model back in\n","restored_saved_model = tf.saved_model.load(model_path)\n","\n","# Verify the restored model gives the same results\n","print (\"Original model results:\", my_model(tf.constant([[1.0, 2.0, 3.0]])))\n","print (\"Restored model results:\", restored_saved_model(tf.constant([[1.0, 2.0, 3.0]])))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
